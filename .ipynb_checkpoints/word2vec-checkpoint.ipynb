{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://zhuanlan.zhihu.com/p/29668368"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    orig_shape = x.shape\n",
    "\n",
    "    # 根据输入类型是矩阵还是向量分别计算softmax\n",
    "    if len(x.shape) > 1:\n",
    "        # 矩阵\n",
    "        tmp = np.max(x,axis=1) # 得到每行的最大值，用于缩放每行的元素，避免溢出\n",
    "        x-=tmp.reshape((x.shape[0],1)) # 使每行减去所在行的最大值（广播运算）\n",
    "\n",
    "        x = np.exp(x) # 第一步，计算所有值以e为底的x次幂\n",
    "        tmp = np.sum(x, axis = 1) # 将每行求和并保存\n",
    "        x /= tmp.reshape((x.shape[0], 1)) # 所有元素除以所在行的元素和（广播运算）\n",
    "    else:\n",
    "        # 向量\n",
    "        tmp = np.max(x) # 得到最大值\n",
    "        x -= tmp # 利用最大值缩放数据\n",
    "        x = np.exp(x) # 对所有元素求以e为底的x次幂\n",
    "        tmp = np.sum(x) # 求元素和\n",
    "        x /= tmp # 求somftmax\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    s = np.true_divide(1, 1 + np.exp(-x)) # 使用np.true_divide进行加法运算\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_grad(s):\n",
    "    ds = s * (1 - s) # 可以证明：sigmoid函数关于输入x的导数等于`sigmoid(x)(1-sigmoid(x))`\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmaxCostAndGradient(predicted, target, outputVectors, dataset=None):\n",
    "\n",
    "    N, D = outputVectors.shape\n",
    "\n",
    "    inner_products = np.dot(outputVectors, predicted)\n",
    "    scores = softmax(inner_products) #(N, 1)     \n",
    "    cost = -np.dot(outputVectors[target], predicted) + np.log(np.sum(np.exp(inner_products)))\n",
    "\n",
    "    gradPred = - outputVectors[target] + np.sum(scores.reshape(-1, 1) * outputVectors, axis=0)\n",
    "    grad = scores.reshape(-1, 1)* np.tile(predicted, (N, 1))\n",
    "    grad[target] -= predicted\n",
    "\n",
    "    return cost, gradPred, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def skipgram(currentWord, C, contextWords, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    \"\"\" Skip-gram model in word2vec \n",
    "    currrentWord -- a string of the current center word \n",
    "    C -- integer, context size \n",
    "    contextWords -- list of no more than 2*C strings, the context words \n",
    "    tokens -- a dictionary that maps words to their indices in the word vector list \"\"\"\n",
    "\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "    N, D = inputVectors.shape\n",
    "\n",
    "    center_index = tokens[currentWord]\n",
    "    predicted = inputVectors[center_index]\n",
    "    for word in contextWords:\n",
    "        target = tokens[word]\n",
    "        cur_cost, cur_gradIn, cur_gradOut = word2vecCostAndGradient(predicted, target, outputVectors, dataset)\n",
    "        cost += cur_cost  # loss\n",
    "        gradIn[center_index] += cur_gradIn\n",
    "        gradOut += cur_gradOut\n",
    "\n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbow(currentWord, C, contextWords, tokens, inputVectors, outputVectors, dataset, word2vecCostAndGradient=softmaxCostAndGradient):\n",
    "    cost = 0.0\n",
    "    gradIn = np.zeros(inputVectors.shape)\n",
    "    gradOut = np.zeros(outputVectors.shape)\n",
    "    N, D = inputVectors.shape\n",
    "\n",
    "    target = tokens[currentWord]\n",
    "    contextWords_vectors = np.array([inputVectors[tokens[word]] for word in contextWords])\n",
    "    predicted = np.sum(contextWords_vectors, axis = 0)\n",
    "    cost, gradpred, grad = word2vecCostAndGradient(predicted, target, outputVectors, dataset)\n",
    "\n",
    "    for word in contextWords:\n",
    "        gradIn[tokens[word]] += gradpred\n",
    "    gradOut = grad\n",
    "\n",
    "    return cost, gradIn, gradOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputVectors = np.random.randn(5, 3) # 输入矩阵，语料库中字母的数量是5，我们使用3维向量表示一个字母\n",
    "outputVectors = np.random.randn(5, 3) # 输出矩阵\n",
    "\n",
    "sentence = ['a', 'e', 'd', 'b', 'd', 'c','d', 'e', 'e', 'c', 'a'] # 句子\n",
    "centerword = 'c' # 中心字母\n",
    "context = ['a', 'e', 'd', 'd', 'd', 'd', 'e', 'e', 'c', 'a'] # 上下文字母\n",
    "tokens = dict([(\"a\", 0), (\"b\", 1), (\"c\", 2), (\"d\", 3), (\"e\", 4)]) # 用于映射字母在输入输出矩阵中的索引\n",
    "\n",
    "c, gin, gout = skipgram(centerword, context, tokens, inputVectors, outputVectors)\n",
    "step = 0.01 #更新步进\n",
    "print('原始输入矩阵:\\n',inputVectors)\n",
    "print('原始输出矩阵:\\n',outputVectors)\n",
    "inputVectors -= step * gin # 更行输入词向量矩阵\n",
    "outputVectors -= step * gout\n",
    "print('更新后的输入矩阵:\\n',inputVectors)\n",
    "print('更新后的输出矩阵:\\n',outputVectors)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
