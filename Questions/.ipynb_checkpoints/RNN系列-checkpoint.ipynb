{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献\n",
    "\n",
    "https://www.jiqizhixin.com/articles/2018-12-14-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "\n",
    "- [RNN](#RNN)\n",
    "- [EncoderDecoder即Seq2Seq](#EncoderDecoder即Seq2Seq)\n",
    "- [Attention](#Attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![121](../imgs/121.png)\n",
    "最基本的单层网络，输入是x，经过变换Wx+b和激活函数f得到输出y。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 to n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以处理的问题有：\n",
    "\n",
    "- 从图像生成文字（image caption），此时输入的X就是图像的特征，而输出的y序列就是一段句子，就像看图说话等\n",
    "- 从类别生成语音或音乐等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 圆圈或方块表示的是向量。\n",
    "- 一个箭头就表示对该向量做一次变换。如上图中h0和x分别有一个箭头连接，就表示对h0和x各做了一次变换。\n",
    "\n",
    "![](../imgs/rnn12n.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n to n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 计算视频中每一帧的分类标签。因为要对每一帧进行计算，因此输入和输出序列等长。\n",
    "- 输入为字符，输出为下一个字符的概率。这就是著名的Char RNN（详细介绍请参考：The Unreasonable Effectiveness of Recurrent Neural Networks，Char RNN可以用来生成文章，诗歌，甚至是代码，非常有意思）。\n",
    "\n",
    "输入输出都是等长序列，经典RNN结构：\n",
    "\n",
    "![](../imgs/rnn4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n to 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 处理序列分类问题，输入一段文字判别所属类别。情感分析、文档分类等。\n",
    "\n",
    "要处理的问题输入是一个序列，输出是一个单独的值而不是序列，在最后一个h上进行输出变换：\n",
    "\n",
    "![](../imgs/rnnn21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EncoderDecoder即Seq2Seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n to m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoder-Decoder结构不限制输入和输出的序列长度.\n",
    "\n",
    "- 机器翻译：Encoder-Decoder的最经典应用，事实上这结构就是在机器翻译领域最先提出的。\n",
    "- 文本摘要：输入是一段文本序列，输出是这段文本序列的摘要序列。\n",
    "- 阅读理解：将输入的文章和问题分别编码，再对其进行解码得到问题的答案。\n",
    "- 语音识别：输入是语音信号序列，输出是文字序列。\n",
    "\n",
    "\n",
    "### Encoder-Decoder 框架\n",
    "Encoder-Decoder 不是一个具体的模型，是一种框架。\n",
    "\n",
    "- Encoder：将 input序列 →转成→ **固定长度的向量**\n",
    "- Decoder：将 固定长度的向量 →转成→ output序列\n",
    "- Encoder 与 Decoder 可以彼此独立使用，实际上经常一起使用\n",
    "\n",
    "因为最早出现的机器翻译领域，最早广泛使用的转码模型是RNN。其实模型可以是 CNN /RNN /BiRNN /LSTM /GRU /…\n",
    "\n",
    "### Encoder-Decoder 缺点\n",
    "\n",
    "- 最大的局限性：编码和解码之间的**唯一联系是固定长度的语义向量c**\n",
    "- 编码要把**整个序列的信息压缩进一个固定长度的语义向量c**\n",
    "- 语义向量**c无法完全表达整个序列的信息**\n",
    "- 先输入的内容携带的信息，会**被后输入的信息稀释掉**，或者被覆盖掉\n",
    "- 输入序列越长，这样的现象越严重，这样使得在**Decoder解码时一开始就没有获得足够的输入序列信息**，解码效果会打折扣"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于输入与输出不等长，所以**Encoder-Decoder结构先将输入数据编码成一个上下文语义向量c：**\n",
    "\n",
    "![](../imgs/rnnn2m.png)\n",
    "\n",
    "如上图所示，**语义向量C**有多种表达方式。得到C后，用另一个RNN网络(Decoder)对C进行解码。在Decoder中，`h0=c`:\n",
    "\n",
    "![](../imgs/rnnn2m2.png)\n",
    "\n",
    "或者\n",
    "\n",
    "![](../imgs/rnnn2m3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了弥补基础的 Encoder-Decoder 的局限性，提出了attention机制。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention机制通过**在每个时间输入不同的c**来解决Encoder-Decoder的问题，下图是带有Attention机制的Decoder(encoder没画出来)：\n",
    "\n",
    "![](../imgs/rnnatt1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**每一个c会自动去选取与当前所要输出的y最合适的上下文信息。**具体来说，我们用$a_{ij}$衡量**Encoder**中第j阶段的$h_j$和解码时第i阶段的相关性，最终**Decoder**中第i阶段的输入的上下文信息 $c_i$就来自于所有 $h_j$ 对 $a_{ij}$ 的加权和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
