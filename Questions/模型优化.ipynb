{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考文献\n",
    "\n",
    "- [1] https://mp.weixin.qq.com/s/RPVzp2gOvufMt2SymxGMVw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目录\n",
    "\n",
    "- [1. 训练误差和泛化误差](#1-训练误差和泛化误差)\n",
    "- [2. 该如何选择模型](#2-该如何选择模型)\n",
    "  - [2.1 验证数据集](#21-验证数据集)\n",
    "  - [2.2 K 折交叉验证](#22-k-折交叉验证)\n",
    "- [3. ⽋拟合和过拟合](#3-⽋拟合和过拟合)\n",
    "    - [防止过拟合](#防止过拟合)\n",
    "    - [训练数据集大小](#训练数据集大小)\n",
    "        - [数据增强](#数据增强)\n",
    "    - [正则化](#正则化)\n",
    "        - [L1和L2正则化](#L1和L2正则化)\n",
    "        - [Dropout](#Dropout)\n",
    "        - [Drop Connect](#Drop-Connect)\n",
    "        - [SpatialDropout](#SpatialDropout)\n",
    "- [4. 梯度消失/梯度爆炸（Vanishing / Exploding gradients）](#4-梯度消失梯度爆炸vanishing--exploding-gradients)\n",
    "- [5. 随机梯度下降法(SGD)](#5-随机梯度下降法sgd)\n",
    "  - [5.1 mini-batch梯度下降](#51-mini-batch梯度下降)\n",
    "  - [5.2 调节 Batch_Size 对训练效果影响到底如何？](#52-调节-batch_size-对训练效果影响到底如何)\n",
    "\n",
    "- [7. 优化算法](#7-优化算法)\n",
    "  - [7.1 动量法](#71-动量法)\n",
    "  - [7.2 AdaGrad算法](#72-adagrad算法)\n",
    "  - [7.3 RMSProp算法](#73-rmsprop算法)\n",
    "  - [7.4 AdaDelta算法](#74-adadelta算法)\n",
    "  - [7.5 Adam算法](#75-adam算法)\n",
    "  - [7.6 局部最优--鞍点问题](#76-局部最优--鞍点问题)\n",
    "- [8. 如何解决训练样本少的问题](#8-如何解决训练样本少的问题)\n",
    "- [9. 如何提升模型的稳定性？](#9-如何提升模型的稳定性)\n",
    "- [10. 有哪些改善模型的思路](#10-有哪些改善模型的思路)\n",
    "- [11. 如何提高深度学习系统的性能](#11-如何提高深度学习系统的性能)\n",
    "- [12. 参考文献](#12-参考文献)\n",
    "\n",
    "- [确定数据](#确定数据)\n",
    "- [是否使用预训练模型](#是否使用预训练模型)\n",
    "- [归一化层（normalization-layers）](#归一化层（normalization-layers）)\n",
    "\n",
    "\n",
    "- [捕捉目标上下文信息](#捕捉目标上下文信息)\n",
    "- [数据不平衡](#数据不平衡)\n",
    "- [测试网络容量](#测试网络容量)\n",
    "- [学习率](#学习率)\n",
    "    - [如何确定初始学习率？](#如何确定初始学习率？)\n",
    "    - [如何确定衰减的步数?](#如何确定衰减的步数?)\n",
    "- [Mini-Batch-Size](#Mini-Batch-Size)\n",
    "- [early stopping](#early-stopping)\n",
    "- [网络层数](#网络层数)\n",
    "- [权值初始化](#权值初始化)\n",
    "- [](#)\n",
    "- [](#)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 训练误差和泛化误差\n",
    "\n",
    "机器学习模型在训练数据集和测试数据集上的表现。如果你改变过实验中的模型结构或者超参数，你也许发现了：当模型在训练数据集上更准确时，它在测试数据集上却不⼀定更准确。这是为什么呢？\n",
    "\n",
    "因为存在着训练误差和泛化误差：\n",
    "\n",
    "- **训练误差：**模型在训练数据集上表现出的误差。\n",
    "- **泛化误差：**模型在任意⼀个测试数据样本上表现出的误差的期望，并常常通过测试数据集上的误差来近似。\n",
    "\n",
    "训练误差的期望小于或等于泛化误差。也就是说，⼀般情况下，由训练数据集学到的模型参数会使模型在训练数据集上的表现优于或等于在测试数据集上的表现。**由于⽆法从训练误差估计泛化误差，⼀味地降低训练误差并不意味着泛化误差⼀定会降低。**\n",
    "\n",
    "**机器学习模型应关注降低泛化误差。**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 该如何选择模型\n",
    "\n",
    "在机器学习中，通常需要评估若⼲候选模型的表现并从中选择模型。这⼀过程称为模型选择（model selection）。可供选择的候选模型可以是有着不同超参数的同类模型。以多层感知机为例，我们可以选择隐藏层的个数，以及每个隐藏层中隐藏单元个数和激活函数。为了得到有效的模型，我们通常要在模型选择上下⼀番功夫。\n",
    "\n",
    "## 2.1 验证数据集\n",
    "\n",
    "从严格意义上讲，测试集只能在所有超参数和模型参数选定后使⽤⼀次。**不可以使⽤测试数据选择模型，如调参。**由于⽆法从训练误差估计泛化误差，因此也**不应只依赖训练数据选择模型**。鉴于此，我们可以**预留⼀部分在训练数据集和测试数据集以外的数据来进⾏模型选择**。这部分数据被称为**验证数据集**，简称验证集（validation set）。例如，我们可以从给定的训练集中随机选取⼀小部分作为验证集，而将剩余部分作为真正的训练集。\n",
    "\n",
    "可以通过预留这样的验证集来进行模型选择，判断验证集在模型中的表现能力。\n",
    "\n",
    "## 2.2 K 折交叉验证\n",
    "\n",
    "由于验证数据集不参与模型训练，当**训练数据不够⽤**时，预留⼤量的验证数据显得太奢侈。⼀种改善的⽅法是K折交叉验证（K-fold cross-validation）。在K折交叉验证中，我们把**原始训练数据集分割成K个不重合的⼦数据集，然后我们做K次模型训练和验证**。每⼀次，我们使⽤⼀个⼦数据集验证模型，并使⽤其他K − 1个⼦数据集来训练模型。在这K次训练和验证中，每次⽤来验证模型的⼦数据集都不同。最后，我们对这**K次训练误差和验证误差分别求平均**。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 3. 欠拟合和过拟合\n",
    "\n",
    "- **欠拟合：**模型⽆法得到较低的训练误差。\n",
    "- **过拟合：**是模型的训练误差远小于它在测试数据集上的误差。\n",
    "\n",
    "给定训练数据集，\n",
    "\n",
    "- 如果模型的复杂度过低，很容易出现⽋拟合；\n",
    "- 如果模型复杂度过⾼，很容易出现过拟合。\n",
    "\n",
    "应对⽋拟合和过拟合的⼀个办法是针对数据集选择合适复杂度的模型。\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-18_21-7-0.png)\n",
    "\n",
    "\n",
    "## 防止过拟合\n",
    "\n",
    "模型参数数量 > 训练数据数量 ->产生过拟合\n",
    "\n",
    "防止过拟合：\n",
    "\n",
    "1. 数据增强\n",
    "2. L1正则化\n",
    "3. L2正则化\n",
    "4. Dropout\n",
    "5. DropoutConnet\n",
    "6. early stopping\n",
    "7. 增加训练数据（推荐算法常用）\n",
    "8. 权值衰减：weight decay参数可以直接设置为:Batchsize/epoch,不需要过度调整。\n",
    "\n",
    "### 训练数据集大小\n",
    "\n",
    "影响⽋拟合和过拟合的另⼀个重要因素是**训练数据集的⼤小**。⼀般来说，如果训练数据集中样本数过少，特别是⽐模型参数数量（按元素计）更少时，过拟合更容易发⽣。此外，泛化误差不会随训练数据集⾥样本数量增加而增⼤。因此，在计算资源允许的范围之内，我们通常希望训练数据集⼤⼀些，特别是在模型复杂度较⾼时，例如层数较多的深度学习模型。\n",
    "\n",
    "#### 数据增强\n",
    "\n",
    "提升算法性能，增加数据数量。\n",
    "\n",
    "- 给训练数据添加转换、扰动增加训练数据，包括：水平、垂直翻转，剪裁，色彩变换，扩展，旋转等\n",
    "\n",
    "### 正则化\n",
    "\n",
    "应对过拟合问题的常⽤⽅法：权重衰减（weight decay），权重衰减等价于L2范数正则化（regularization）。正则化通过为模型损失函数添加惩罚项使学出的模型参数值较小，是应对过拟合的常⽤⼿段。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### L1和L2正则化\n",
    " \n",
    " - L1正则化：向loss函数中添加可训练参数绝对值总和\n",
    " \n",
    "     - 在非常稀疏的向量上计算效率很低\n",
    "     - 可以获得稀疏模型，使得大部分参数稀疏，会把不重要的特征直接置零（可减少存储空间）\n",
    "     - loss的原点不可导\n",
    "     - 解不唯一\n",
    "     - 强制大部分输入特征对应的权重为0，一般用于特征选择\n",
    " \n",
    " - L2正则化：向loss函数中添加可训练参数的平方总和\n",
    "     - 唯一解\n",
    "     - 计算方便，实现简单\n",
    "     - 几乎不会把参数置零，一般用于学习得到较小的权重\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dropout\n",
    "\n",
    "Bagging 是通过**结合多个模型降低泛化误差**的技术，主要的做法是**分别训练几个不同的模型，然后让所有模型表决测试样例的输出**。而 Dropout 可以被认为是**集成了大量深层神经网络的 Bagging 方法**，因此它提供了一种*廉价的 Bagging 集成近似方法*，能够训练和评估的神经网络。\n",
    "\n",
    "Dropout 指**暂时丢弃一部分神经元及其连接**。随机丢弃神经元可以防止过拟合，同时**指数级、高效地连接不同网络架构**。神经元被丢弃的概率为 1 − p，**减少神经元之间的共适应**。隐藏层通常以 0.5 的概率丢弃神经元。使用完整网络（每个节点的输出权重为 p）对所有 2^n 个 dropout 神经元的样本平均值进行近似计算。Dropout 显著降低了过拟合，同时通过避免在训练数据上的训练节点提高了算法的学习速度。\n",
    "\n",
    "- 增加训练时间\n",
    "- dropout对小数据防止过拟合有很好的效果,值一般设为0.5,小数据上dropout+sgd效果更好.\n",
    "    - 注意：对于小数据集，即使使用BN，还是需要使用dropout。对于大数据集，使用BN就没必要使用dropout了"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回忆⼀下，“多层感知机”描述了⼀个单隐藏层的多层感知机。其中输⼊个数为4，隐藏单元个数为5，且隐藏单元hi（i = 1, . . . , 5）的计算表达式为：\n",
    "\n",
    "![](https://latex.codecogs.com/gif.latex?h_i=\\phi(x_1w_{1i}+x_2w_{2i}+x_3w_{3i}+x_4w_{4i}+b_i))\n",
    "\n",
    "这⾥ϕ是激活函数，x1, . . . , x4是输⼊，隐藏单元i的权重参数为w1i, . . . , w4i，偏差参数为bi。当对该隐藏层使⽤丢弃法时，该层的隐藏单元将有⼀定概率被丢弃掉。设丢弃概率为p，那么有p的概率hi会被清零，有1 − p的概率hi会除以1 − p做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量ξi为0和1的概率分别为p和1 − p。使⽤丢弃法时我们计算新的隐藏单元 ![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_10-38-57.png)。\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_10-42-10.png)\n",
    "\n",
    "由于E(ξi) = 1 − p，因此：\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_10-43-31.png)\n",
    "\n",
    "**即丢弃法不改变其输⼊的期望值。**让我们对隐藏层使⽤丢弃法，⼀种可能的结果如下图所⽰，其中h2和h5被清零。这时输出值的计算不再依赖h2和h5，在反向传播时，与这两个隐藏单元相关的权重的梯度均为0。由于在训练中隐藏层神经元的丢弃是随机的，即h1, . . . , h5都有可能被清零，输出层的计算⽆法过度依赖h1, . . . , h5中的任⼀个，从而在训练模型时起到正则化的作⽤，并可以⽤来应对过拟合。在测试模型时，我们为了拿到更加确定性的结果，⼀般不使⽤丢弃法。\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-18_21-24-40.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Connect\n",
    "\n",
    "是dropout的一般化。\n",
    "\n",
    "drop connect将网络架构权重的一个随机选择自己置零，而dropout是对每层的激活值的子集置零。\n",
    "\n",
    "- 对于每个单元，接收的输入数据都是上一层输出的子集，所以两者都有泛化性能。\n",
    "- 二者在模型中引入稀疏性，dropout是输出向量稀疏性，drop connect是权重稀疏性。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SpatialDropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 用于较低层而不是较高层\n",
    "- 两个+conv层对相同的输入进行计算，得到的feature map后使用spatial dropout。\n",
    "\n",
    "因为多个卷积层从相同的输入提取的feature map可能是相关的，所以用spatial drop删除相关特征，防止网络过拟合。\n",
    "\n",
    "![spatialdropout](../imgs/spatialdropout.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. 梯度消失/梯度爆炸（Vanishing / Exploding gradients）\n",
    "\n",
    "训练神经网络，尤其是深度神经所面临的一个问题就是梯度消失或梯度爆炸，也就是你训练神经网络的时候，导数或坡度有时会变得非常大，或者非常小，甚至于以指数方式变小，这加大了训练的难度。\n",
    "\n",
    "本质上，梯度消失和爆炸是一种情况。在深层网络中，由于网络过深，如果初始得到的梯度过小，或者传播途中在某一层上过小，则在之后的层上得到的梯度会越来越小，即产生了梯度消失。梯度爆炸也是同样的。一般地，不合理的初始化以及激活函数，如sigmoid等，都会导致梯度过大或者过小，从而引起消失/爆炸。\n",
    "\n",
    "**解决方案**\n",
    "\n",
    "1. **预训练加微调**\n",
    "\n",
    "   其基本思想是每次训练一层隐节点，训练时将上一层隐节点的输出作为输入，而本层隐节点的输出作为下一层隐节点的输入，此过程就是逐层“预训练”（pre-training）；在预训练完成后，再对整个网络进行“微调”（fine-tunning）。\n",
    "\n",
    "   此方法有一定的好处，但是目前应用的不是很多了。\n",
    "\n",
    "2. **梯度剪切、正则**\n",
    "\n",
    "   梯度剪切这个方案主要是针对梯度爆炸提出的，其思想是设置一个梯度剪切阈值，然后更新梯度的时候，如果梯度超过这个阈值，那么就将其强制限制在这个范围之内。这可以防止梯度爆炸。\n",
    "\n",
    "   另外一种解决梯度爆炸的手段是采用权重正则化（weithts regularization）比较常见的是L1和L2正则。\n",
    "\n",
    "3. **ReLu、leakReLu等激活函数**\n",
    "\n",
    "   ReLu：其函数的导数在正数部分是恒等于1，这样在深层网络中，在激活函数部分就不存在导致梯度过大或者过小的问题，缓解了梯度消失或者爆炸。同时也方便计算。当然，其也存在存在一些缺点，例如过滤到了负数部分，导致部分信息的丢失，输出的数据分布不在以0为中心，改变了数据分布。\n",
    "\n",
    "   leakrelu：就是为了解决relu的0区间带来的影响，其数学表达为：leakrelu=max(k*x,0)其中k是leak系数，一般选择0.01或者0.02，或者通过学习而来。\n",
    "\n",
    "4. **Batch Normalization**\n",
    "\n",
    "   Batch Normalization是深度学习发展以来提出的最重要的成果之一了，目前已经被广泛的应用到了各大网络中，具有加速网络收敛速度，提升训练稳定性的效果，Batch Normalization本质上是解决反向传播过程中的梯度问题。Batch Normalization，简称BN，即批规范化，通过规范化操作将输出信号x规范化到均值为0，方差为1保证网络的稳定性。\n",
    "\n",
    "   - 有一些从 0 到 1 而不是从 1 到 1000 的特征值，通过归一化所有的输入特征值𝑥，以获得类似范围的值，可以加速学习。所以 Batch 归一化起的作用的原因，直观的一点就是，它在做类似的工作，但不仅仅对于这里的输入值，还有隐藏单元的值。\n",
    "   - 它可以使权重比你的网络更滞后或更深层，比如，第 10 层的权重更能经受得住变化。\n",
    "\n",
    "5. **残差结构**\n",
    "\n",
    "   残差的方式，能使得深层的网络梯度通过跳级连接路径直接返回到浅层部分，使得网络无论多深都能将梯度进行有效的回传。\n",
    "\n",
    "6. **LSTM**\n",
    "\n",
    "   LSTM全称是长短期记忆网络（long-short term memory networks），是不那么容易发生梯度消失的，主要原因在于LSTM内部复杂的“门”(gates)。在计算时，将过程中的梯度进行了抵消。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 随机梯度下降法(SGD)\n",
    "\n",
    "## 5.1 mini-batch梯度下降\n",
    "\n",
    "你可以把训练集分割为小一点的子集训练，这些子集被取名为 **mini-batch**，假设每一个子集中只有 1000 个样本，那么把其中的𝑥 (1)到𝑥 (1000)取出来，将其称为第一个子训练集，也叫做 **mini-batch**，然后你再取出接下来的 1000 个样本，从𝑥 (1001)到𝑥 (2000)，然后再取 1000个样本，以此类推。\n",
    "\n",
    "在训练集上运行 **mini-batch** 梯度下降法，你运行 for t=1……5000，因为我们有5000个各有 1000 个样本的组，在 **for** 循环里你要做得基本就是对𝑋 {𝑡}和𝑌 {𝑡}执行一步梯度下降法。 \n",
    "\n",
    "- batch_size=1，就是SGD。\n",
    "- batch_size=n，就是mini-batch\n",
    "- batch_size=m，就是batch\n",
    "\n",
    "其中1<n<m，m表示整个训练集大小。\n",
    "\n",
    "**优缺点：**\n",
    "\n",
    "- batch：相对噪声低些，幅度也大一些，你可以继续找最小值。 \n",
    "- SGD：大部分时候你向着全局最小值靠近，有时候你会远离最小值，因为那个样本恰好给你指的方向不对，因此随机梯度下降法是有很多噪声的，平均来看，它最终会靠近最小值，不过有时候也会方向错误，因为随机梯度下降法永远不会收敛，而是会一直在最小值附近波动。一次性只处理了一个训练样本，这样效率过于低下。\n",
    "- mini-batch：实践中最好选择不大不小的 **mini-batch**，得到了大量向量化，效率高，收敛快。\n",
    "\n",
    "首先，如果训练集较小，直接使用 **batch** 梯度下降法，这里的少是说小于 2000 个样本。一般的 **mini-batch** 大小为 64 到 512，考虑到电脑内存设置和使用的方式，如果 **mini-batch** 大小是 2 的𝑛次方，代码会运行地快一些。\n",
    "\n",
    "## 5.2 调节 Batch_Size 对训练效果影响到底如何？\n",
    "\n",
    "1. Batch_Size 太小，模型表现效果极其糟糕(error飙升)。\n",
    "2. 随着 Batch_Size 增大，处理相同数据量的速度越快。\n",
    "3. 随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。\n",
    "4. 由于上述两种因素的矛盾， Batch_Size 增大到某个时候，达到时间上的最优。\n",
    "5. 由于最终收敛精度会陷入不同的局部极值，因此 Batch_Size 增大到某些时候，达到最终收敛精度上的最优。 \n",
    "\n",
    "决定了网络的更新方向考虑的样本数\n",
    "- 大，因为并行，加快网络训练速度\n",
    "- 小，网络更新次数增多\n",
    "- 数量可以根据GPU显存大小确定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 优化算法\n",
    "\n",
    "## 7.1 动量法\n",
    "\n",
    "在每次迭代中，梯度下降根据⾃变量当前位置，沿着当前位置的梯度更新⾃变量。然而，如果⾃变量的\n",
    "迭代⽅向仅仅取决于⾃变量当前位置，这可能会带来⼀些问题。\n",
    "\n",
    "让我们考虑⼀个输⼊和输出分别为⼆维向量x = [x1, x2]⊤和标量的⽬标函数 ![](https://latex.codecogs.com/gif.latex?f(x)=0.1x_1^2+2x_2^2)。，这⾥将![](https://latex.codecogs.com/gif.latex?x_1^2)系数从1减小到了0.1。下⾯实现基于这个⽬标函数的梯度下降，并演⽰使⽤学习率为0.4时⾃变量的迭代轨迹。\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-18_22-24-50.png)\n",
    "\n",
    "可以看到，同⼀位置上，⽬标函数在竖直⽅向（x2轴⽅向）⽐在⽔平⽅向（x1轴⽅向）的斜率的绝对值更⼤。**因此，给定学习率，梯度下降迭代⾃变量时会使⾃变量在竖直⽅向⽐在⽔平⽅向移动幅度更⼤。**那么，我们需要⼀个较小的学习率从而避免⾃变量在竖直⽅向上越过⽬标函数最优解。然而，这会造成⾃变量在⽔平⽅向上朝最优解移动变慢。\n",
    "\n",
    "**动量法的提出是为了解决梯度下降的上述问题。**由于小批量随机梯度下降⽐梯度下降更为⼴义，本章后续讨论将沿⽤“小批量随机梯度下降”⼀节中时间步t的小批量随机梯度gt的定义。设时间步t的⾃变量为xt，学习率为ηt。在时间步0，动量法创建速度变量v0，并将其元素初始化成0。在时间步t > 0，动量法对每次迭代的步骤做如下修改：\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_10-46-51.png)\n",
    "\n",
    "其中，动量超参数γ满⾜0 ≤ γ < 1。当γ = 0时，动量法等价于小批量随机梯度下降。在梯度下降时候使用动量法后的迭代轨迹：\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-18_22-34-20.png)\n",
    "\n",
    "可以看到使⽤较小的学习率η = 0.4和动量超参数γ = 0.5时，动量法在竖直⽅向上的移动更加平滑，且在⽔平⽅向上更快逼近最优解。\n",
    "\n",
    "所以，在动量法中，⾃变量在各个⽅向上的移动幅度不仅取决当前梯度，还取决于过去的各个梯度在各个⽅向上是否⼀致。在本节之前⽰例的优化问题中，所有梯度在⽔平⽅向上为正（向右），而在竖直⽅向上时正（向上）时负（向下）。这样，我们就可以使⽤较⼤的学习率，从而使⾃变量向最优解更快移动。\n",
    "\n",
    "## 7.2 AdaGrad算法\n",
    "\n",
    "优化算法中，⽬标函数⾃变量的每⼀个元素在相同时间步都使⽤同⼀个学习率来⾃我迭代。在“动量法”⾥我们看到当x1和x2的梯度值有较⼤差别时，需要选择⾜够小的学习率使得⾃变量在梯度值较⼤的维度上不发散。但这样会导致⾃变量在梯度值较小的维度上迭代过慢。动量法依赖指数加权移动平均使得⾃变量的更新⽅向更加⼀致，从而降低发散的可能。**本节我们介绍AdaGrad算法，它根据⾃变量在每个维度的梯度值的⼤小来调整各个维度上的学习率，从而避免统⼀的学习率难以适应所有维度的问题。**\n",
    "\n",
    "AdaGrad算法会使⽤⼀个小批量随机梯度gt按元素平⽅的累加变量st。在时间步0，AdaGrad将s0中每个元素初始化为0。在时间步t，⾸先将小批量随机梯度gt按元素平⽅后累加到变量st：\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_10-49-20.png)\n",
    "\n",
    "其中⊙是按元素相乘。接着，我们将⽬标函数⾃变量中每个元素的学习率通过按元素运算重新调整⼀下：\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_10-50-24.png)\n",
    "\n",
    "其中η是学习率，ϵ是为了维持数值稳定性而添加的常数，如10的-6次方。这⾥开⽅、除法和乘法的运算都是按元素运算的。这些按元素运算使得⽬标函数⾃变量中每个元素都分别拥有⾃⼰的学习率。\n",
    "\n",
    "需要强调的是，小批量随机梯度按元素平⽅的累加变量st出现在学习率的分⺟项中。因此，\n",
    "\n",
    "- 如果⽬标函数有关⾃变量中某个元素的偏导数⼀直都较⼤，那么该元素的学习率将下降较快；\n",
    "- 反之，如果⽬标函数有关⾃变量中某个元素的偏导数⼀直都较小，那么该元素的学习率将下降较慢。\n",
    "\n",
    "然而，由于st⼀直在累加按元素平⽅的梯度，⾃变量中每个元素的学习率在迭代过程中⼀直在降低（或不变）。**所以，当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到⼀个有⽤的解。**\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-18_22-48-46.png)\n",
    "\n",
    "## 7.3 RMSProp算法\n",
    "\n",
    "当学习率在迭代早期降得较快且当前解依然不佳时，AdaGrad算法在迭代后期由于学习率过小，可能较难找到⼀个有⽤的解。为了解决这⼀问题，RMSProp算法对AdaGrad算法做了⼀点小小的修改。\n",
    "\n",
    "不同于AdaGrad算法⾥状态变量st是截⾄时间步t所有小批量随机梯度gt按元素平⽅和，RMSProp算法将这些梯度按元素平⽅做指数加权移动平均。具体来说，给定超参数0 ≤ γ < 1，RMSProp算法在时间步t > 0计算：\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_10-51-9.png)\n",
    "\n",
    "和AdaGrad算法⼀样，RMSProp算法将⽬标函数⾃变量中每个元素的学习率通过按元素运算重新调整，然后更新⾃变量：\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_10-52-3.png)\n",
    "\n",
    "其中η是学习率，ϵ是为了维持数值稳定性而添加的常数，如10的-6次方。因为RMSProp算法的状态变量st是对平⽅项gt ⊙ gt的指数加权移动平均，**所以可以看作是最近1/(1 − γ)个时间步的小批量随机梯度平⽅项的加权平均。如此⼀来，⾃变量每个元素的学习率在迭代过程中就不再⼀直降低（或不变）。**\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-18_22-57-39.png)\n",
    "\n",
    "## 7.4 AdaDelta算法\n",
    "\n",
    "除了RMSProp算法以外，另⼀个常⽤优化算法AdaDelta算法也针对AdaGrad算法在迭代后期可能较难找到有⽤解的问题做了改进。有意思的是，AdaDelta算法没有学习率这⼀超参数。\n",
    "\n",
    "AdaDelta算法也像RMSProp算法⼀样，使⽤了小批量随机梯度gt按元素平⽅的指数加权移动平均变量st。在时间步0，它的所有元素被初始化为0。给定超参数0 ≤ ρ < 1（对应RMSProp算法中的γ），在时间步t > 0，同RMSProp算法⼀样计算：\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_10-52-53.png)\n",
    "\n",
    "与RMSProp算法不同的是，AdaDelta算法还维护⼀个额外的状态变量∆xt，其元素同样在时间步0时被初始化为0。我们使⽤∆xt−1来计算⾃变量的变化量：\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_10-53-39.png)\n",
    "\n",
    "最后，我们使⽤∆xt来记录⾃变量变化量 ![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_10-56-36.png)按元素平⽅的指数加权移动平均：\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_10-57-14.png)\n",
    "\n",
    "可以看到，如不考虑ϵ的影响，AdaDelta算法与RMSProp算法的不同之处在于使⽤ ![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_10-58-43.png)来替代超参数η。\n",
    "\n",
    "## 7.5 Adam算法\n",
    "\n",
    "Adam算法在RMSProp算法基础上对小批量随机梯度也做了指数加权移动平均。\n",
    "\n",
    "Adam算法使⽤了动量变量vt和RMSProp算法中小批量随机梯度按元素平⽅的指数加权移动平均变量st，并在时间步0将它们中每个元素初始化为0。给定超参数0 ≤ β1 < 1（算法作者建议设为0.9），时间步t的动量变量vt即小批量随机梯度gt的指数加权移动平均：\n",
    "\n",
    "![](https://latex.codecogs.com/gif.latex?v_t=\\beta_1v_{t-1}+(1-\\beta_1)g_t)\n",
    "\n",
    "和RMSProp算法中⼀样，给定超参数0 ≤ β2 < 1（算法作者建议设为0.999），将小批量随机梯度按元素平⽅后的项gt ⊙ gt做指数加权移动平均得到st：\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_10-59-43.png)\n",
    "\n",
    "由于我们将 v0 和 s0 中的元素都初始化为 0，在时间步 t 我们得到 ![](https://latex.codecogs.com/gif.latex?v_t=(1-\\beta_1)\\sum_{i=1}^t\\beta_1^{t-i}g_i)。将过去各时间步小批量随机梯度的权值相加，得到 ![](https://latex.codecogs.com/gif.latex?(1-\\beta_1)\\sum_{i=1}^t\\beta_1^{t-i}=1-\\beta_1^t)。需要注意的是，当 t 较小时，过去各时间步小批量随机梯度权值之和会较小。例如，当β1 = 0.9时，v1 = 0.1g1。为了消除这样的影响，对于任意时间步 t，我们可以将 vt 再除以 ![](https://latex.codecogs.com/gif.latex?1-\\beta_1^t)，从而使过去各时间步小批量随机梯度权值之和为1。这也叫作偏差修正。在Adam算法中，我们对变量 vt 和 st 均作偏差修正：\n",
    "\n",
    "![](https://latex.codecogs.com/gif.latex?\\check{v}_t=\\frac{v_t}{1-\\beta_1^t})\n",
    "\n",
    "![](https://latex.codecogs.com/gif.latex?\\check{s}_t=\\frac{s_t}{1-\\beta_2^t})\n",
    "\n",
    "接下来，Adam算法使⽤以上偏差修正后的变量***v***ˆ*t*和***s***ˆ*t*，将模型参数中每个元素的学习率通过按元素运算重新调整： \n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_11-2-20.png)\n",
    "\n",
    "其中*η*是学习率，*ϵ*是为了维持数值稳定性而添加的常数，如10的-8次方。和AdaGrad算法、RMSProp算法以及AdaDelta算法⼀样，⽬标函数⾃变量中每个元素都分别拥有⾃⼰的学习率。最后，使⽤![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_11-3-50.png)迭代⾃变量： \n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_11-4-23.png)\n",
    "\n",
    "## 7.6 局部最优--鞍点问题\n",
    "\n",
    "一个具有高维度空间的函数，如果梯度为 0，那么在每个方向，它可能是凸函数，也可能是凹函数。如果你在 2 万维空间中，那么想要得到局部最优，所有的 2 万个方向都需要是这样，但发生的机率也许很小，也许是2的-20000次方，你更有可能遇到有些方向的曲线会这样向上弯曲，另一些方向曲线向下弯，而不是所有的都向上弯曲，因此在高维度空间，你更可能碰到鞍点。\n",
    "\n",
    "![](https://gitee.com/kkweishe/images/raw/master/ML/2019-8-19_9-3-31.png)\n",
    "\n",
    "而不会碰到局部最优。**至于为什么会把一个曲面叫做鞍点，**你想象一下，就像是放在马背上的马鞍一样，如果这是马，这是马的头，这就是马的眼睛，画得不好请多包涵，然后你就是骑马的人，要坐在马鞍上，因此这里的这个点，导数为 0 的点，这个点叫做鞍点。我想那确实是你坐在马鞍上的那个点，而这里导数为 0。\n",
    "\n",
    "鞍点中的平稳段是一个问题，这样使得学习十分缓慢，**这也是像 Momentum 或是RMSprop，Adam 这样的算法，能够加速学习算法的地方。**在这些情况下，更成熟的优化算法，如 Adam 算法，能够加快速度，让你尽早往下走出平稳段。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 如何解决训练样本少的问题\n",
    "\n",
    "1. **利用预训练模型进行迁移微调（fine-tuning），**预训练模型通常在特征上拥有很好的语义表达。此时，只需将模型在小数据集上进行微调就能取得不错的效果。CV有ImageNet，NLP有BERT等。\n",
    "2. **数据集进行下采样操作，**使得符合数据同分布。\n",
    "3. 数据集增强、正则或者半监督学习等方式来解决小样本数据集的训练问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. 如何提升模型的稳定性？\n",
    "\n",
    "1. 正则化（L2, L1, dropout）：模型方差大，很可能来自于过拟合。正则化能有效的降低模型的复杂度，增加对更多分布的适应性。\n",
    "2. 前停止训练：提前停止是指模型在验证集上取得不错的性能时停止训练。这种方式本质和正则化是一个道理，能减少方差的同时增加的偏差。目的为了平衡训练集和未知数据之间在模型的表现差异。\n",
    "3. 扩充训练集：正则化通过控制模型复杂度，来增加更多样本的适应性。\n",
    "4. 特征选择：过高的特征维度会使模型过拟合，减少特征维度和正则一样可能会处理好方差问题，但是同时会增大偏差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. 有哪些改善模型的思路\n",
    "\n",
    "1. **数据角度 **\n",
    "\n",
    "   增强数据集。无论是有监督还是无监督学习，数据永远是最重要的驱动力。更多的类型数据对良好的模型能带来更好的稳定性和对未知数据的可预见性。对模型来说，“看到过的总比没看到的更具有判别的信心”。\n",
    "\n",
    "2. **模型角度**\n",
    "\n",
    "   模型的容限能力决定着模型可优化的空间。在数据量充足的前提下，对同类型的模型，增大模型规模来提升容限无疑是最直接和有效的手段。\n",
    "\n",
    "3. **调参优化角度**\n",
    "\n",
    "   如果你知道模型的性能为什么不再提高了，那已经向提升性能跨出了一大步。 超参数调整本身是一个比较大的问题。一般可以包含模型初始化的配置，优化算法的选取、学习率的策略以及如何配置正则和损失函数等等。\n",
    "\n",
    "4. **训练角度**\n",
    "\n",
    "   在越大规模的数据集或者模型上，诚然一个好的优化算法总能加速收敛。但你在未探索到模型的上限之前，永远不知道训练多久算训练完成。所以在改善模型上充分训练永远是最必要的过程。充分训练的含义不仅仅只是增大训练轮数。有效的学习率衰减和正则同样是充分训练中非常必要的手段。\n",
    "\n",
    "# 11. 如何提高深度学习系统的性能\n",
    "\n",
    "1. 提高模型的结构。\n",
    "2. 改进模型的初始化方式，保证早期梯度具有某些有益的性质，或者具备大量的稀疏性，或者利用线性代数原理的优势。\n",
    "3. 择更强大的学习算法。\n",
    "\n",
    "# 12. 参考文献\n",
    "\n",
    "- [动手学--深度学习](http://zh.gluon.ai/)\n",
    "- [吴恩达--深度学习笔记](https://github.com/fengdu78/deeplearning_ai_books)\n",
    "- [GitHub](https://github.com/scutan90/DeepLearning-500-questions)\n",
    "- [百面机器学习](https://www.lanzous.com/i56i24f)\n",
    "\n",
    "------\n",
    "\n",
    "> 作者：[@mantchs](https://github.com/NLP-LOVE/ML-NLP)\n",
    ">\n",
    "> GitHub：[https://github.com/NLP-LOVE/ML-NLP](https://github.com/NLP-LOVE/ML-NLP)\n",
    ">\n",
    "> 欢迎大家加入讨论！共同完善此项目！群号：【541954936】<a target=\"_blank\" href=\"//shang.qq.com/wpa/qunwpa?idkey=863f915b9178560bd32ca07cd090a7d9e6f5f90fcff5667489697b1621cecdb3\"><img border=\"0\" src=\"http://pub.idqqimg.com/wpa/images/group.png\" alt=\"NLP面试学习群\" title=\"NLP面试学习群\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 确定数据\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 确保输入(x)对应于一个label(y)，多分类任务有多个label\n",
    "- 确保预测数据中label与训练label一致，例如one-hot格式，cate格式，数值范围等。之前首焦模型中就用feat作为label导致取值范围不同都默认为负样本，训练失败。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 是否使用预训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 如果问题域中的数据集类似于ImageNet数据集，可以使用预训练模型，如VGG，Resnet，DenseNet，Xception等。建议从较少的网络层开始尝试（例如VGG-16, ResNet-50或densen -121层），获得最佳参数后再尝试同种网络的更多的网络层。\n",
    "\n",
    "- 也可以只使用初始的几层（锁定不更新）提取基本特征（例如锯齿、形状等）高阶层次自定义。\n",
    "\n",
    "- 微调几层（预训练模型中浅层锁定，高层更新），或者数据集较小，只训练最后的分类层。还可以在需要微调（更新）的conv层后加入dropout层，抵抗过拟合。但是**dropout在层次少的模型（数据少）中效果好，高层的（数据多，用了bn就不用dropout了）效果一般**。\n",
    "\n",
    "- 如果数据集和预训练网络的数据集（ImageNet）不相似，则从头构建并训练模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 归一化层（normalization-layers）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 较大`batch size`（>10），使用**批标准化层（BatchNormalization）**\n",
    "- 较小`batch size`（= 1），使用**InstanceNormalization**\n",
    "- 较小bs使用BN层性能会降低\n",
    "- layernorm？\n",
    "- groupnorm？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 捕捉目标上下文信息\n",
    "\n",
    "使用多尺度pooling\n",
    "\n",
    "在ReLU之前使用最大池化来节省一些计算。由于ReLU阈值的值为0：f(x)=max(0,x)和最大池化只有max激活：f(x)=max(x1,x2，…，xi)，使用Conv > MaxPool > ReLU 而不是Conv > ReLU > MaxPool。使用MaxPool > ReLU可以节省一个max 操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据不平衡\n",
    "\n",
    "- 过采样\n",
    "- 欠采样\n",
    "- 类别加权\n",
    "- 使用其他评估指标：\n",
    "    - precision：类别精度，当模型判断一个样本属于该类的情况下，判断结果的可信度\n",
    "    - recall：类别召回率，模型能够检测到该类的比率。\n",
    "    - F1：precision和recall的调和平均值，将一个类的precision和recall结合在一个指标中。\n",
    "    \n",
    "    - 高precision + 高recall：模型能够很好的检测该类\n",
    "    - 高precision + 低recall：模型不能很好的检测该类，但是判断结果是高度可信的\n",
    "    - 低precision + 高recall：模型能很好的检测该类，但是判断结果中包含其他类的样本\n",
    "    - 低precision + 低recall：模型不能很好的检测该类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试网络容量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了确定你的网络容量，尝试用一小部分训练例子来超载你的网络(andrej karpathy的提示)。如果它没有超载，增加你的网络容量。在过拟合后，使用正则化技巧如L1、L2、Dropout或其他技术来对抗过拟合。\n",
    "\n",
    "- L1\n",
    "\n",
    "- L2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 如何确定初始学习率？\n",
    "\n",
    "- 对于具有标准化输入/以[0,1]为界的输入，通常设置为1-10^-6之间，一般为0.1，0.01，0.001，1e-6\n",
    "- 尝试不同的初始learning rate，观察固定步数(1k~1w)的loss的变化\n",
    "- 初始学习率变化： 3倍衰减:0.1->0.03->0.01->0.003->0.001\n",
    "- 如果网络输入没有归一化到0-1:打印出**每层参数**的最大、最小值、均值，以及**回传梯度**的最大、最小值和均值，根据梯度值和参数大小的**量级**决定初始学习率的量级。（**初始学习率的量级一般在参数量级的1e-2~1e-6之间较为合适**）\n",
    "\n",
    "- 微调，请考虑小于1e-3(比如1e-4)的低学习率。\n",
    "- 从头开始训练网络，请考虑一个大于或等于1e-3的学习率\n",
    "\n",
    "## 如何确定衰减的步数?\n",
    "\n",
    "学习率应该随着训练的进行逐步衰减:**初始时较大的学习率能使网络更快的收敛**，随着训练的进行**更小的学习率能使网络收敛到更好的局部极小值点**。\n",
    "\n",
    "实际中看Loss function:\n",
    "\n",
    "1)**固定初始学习率训练**，直到loss function**降的很缓慢或者不降,大致记录下此步数t**\n",
    "\n",
    "2)**重新开始训练**，到**t时**学习率直接**衰减为原来的0.1**(ImageNet网络训练方法)；或者**采用某种衰减策略**$/lambda _t = /frac{/lambda _0}(t^{/alpha}) $,$/lambda (s)=/frac{/lambda _0}{1+s*/lambda _t}$或者直接停止训练early stop\n",
    "\n",
    "- 如果刚开始设计模型不太关注学习率：直接使用自适应学习率的方法Adam,Adagrad等;但这些方法**往往能取得较好的结果，但很难取得很好的结果**。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## early stopping\n",
    "\n",
    "限制模型最小化loss的训练迭代次数，防止训练中过度表达的模型泛化性能差。\n",
    "\n",
    "epoch少 -> 欠拟合（方差较小，偏差较大）\n",
    "\n",
    "epoch大 -> 过拟合（方差较大，偏差较小）\n",
    "\n",
    "设置好停止条件即可。\n",
    "\n",
    "但实际中：\n",
    "\n",
    "- 超参数T一般不设置，直接让训练跑下去，同时输出完整训练周期内训练,验证的loss curve，当验证集Loss不再明显下降时，再训练3~5个epoch就可以停止了。\n",
    "- 稍微复杂点的解决方案:设置patience,validation loss明显下降时增加patience,validation loss连续patience个epoch不再下降时，就stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 损失函数\n",
    "\n",
    "一般根据任务确定，但有一点：输出为softmax时，尽量使用cross-entropy，尽量避免使用Square Error\n",
    "\n",
    "http://jmlr.org/proceedings/papers/v9/glorot10a/glorot10a.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 网络层数\n",
    "\n",
    "对于卷积神经网路而言:\n",
    "\n",
    "- 确定第一层的卷积核数量，根据实际问题设置卷积核大小(3x3,5x5,7x7)\n",
    "- 目标在输入图像中**占比比较大，需要较大的卷积核，或者更深的网络层次，增大卷积神经网络的感受野**\n",
    "- 根据第一层卷积核数量，按照“**图像尺寸减小一半，特征图数量增加一倍**”的设计方法，往上增加卷积层，后面添加2个固定大小的全连接层(1000个神经元)，**每增加一层训练一次计算在测试集上准确率，直到增加网络层数无法再增加准确率为止**\n",
    "- 将每组网络层大的卷积核**(5x5,7x7)全部换为多层的3x3的小卷积核（ 2层3x3卷积核的感受野为5x5,3层3x3的卷积核感受野为7x7）**\n",
    "- 增加每组网络层小的卷积核层数，直到增加卷积核层数无法再增加准确率为止。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 权值初始化\n",
    "\n",
    "- w权重\n",
    "    - 权重初始化为均匀分布和高斯分布没有明显区别，但relu建议采用MSRA Initialization 对于有$m$个输入， $n$个输出的层:\n",
    "    均匀分布:\n",
    "    高斯分布(均值为0):\n",
    "    - stddev=0.01或者0.001（一个小的常数）带来的问题是，可能过小而导致前向传播时梯度消失\n",
    "    - Xavier Initialization:输入和输出方差一致：$stddev = sqrt(1/m)$\n",
    "    - 激活函数为relu，推荐MSRA Initialization:  $stddev = sqrt(2/m)$\n",
    "    \n",
    "- bias偏置\n",
    "    - 一般直接置为0\n",
    "    - 一个单元会控制其他单元能否参与到等式中，比如门操作，在LSTM中将forget_bias=1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
